{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TvyemfDlDmu"
   },
   "source": [
    "### Coursework coding instructions (please also see full coursework spec)\n",
    "\n",
    "Please choose if you want to do either Task 1 or Task 2. You should write your report about one task only.\n",
    "\n",
    "For the task you choose you will need to do two approaches:\n",
    "  - Approach 1, which can use use pre-trained embeddings / models\n",
    "  - Approach 2, which should not use any pre-trained embeddings or models\n",
    "We should be able to run both approaches from the same colab file\n",
    "\n",
    "#### Running your code:\n",
    "  - Your models should run automatically when running your colab file without further intervention\n",
    "  - For each task you should automatically output the performance of both models\n",
    "  - Your code should automatically download any libraries required\n",
    "\n",
    "#### Structure of your code:\n",
    "  - You are expected to use the 'train', 'eval' and 'model_performance' functions, although you may edit these as required\n",
    "  - Otherwise there are no restrictions on what you can do in your code\n",
    "\n",
    "#### Documentation:\n",
    "  - You are expected to produce a .README file summarising how you have approached both tasks\n",
    "\n",
    "#### Reproducibility:\n",
    "  - Your .README file should explain how to replicate the different experiments mentioned in your report\n",
    "\n",
    "Good luck! We are really looking forward to seeing your reports and your model code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LRWFk-kelDoA",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You will need to download any word embeddings required for your code, e.g.:\n",
    "# ! pip install sacremoses\n",
    "# ! pip install transformers\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip -P /vol/bitbucket/rp3317/data/\n",
    "# !unzip /vol/bitbucket/rp3317/data/glove.6B.zip -d /vol/bitbucket/rp3317/data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LRWFk-kelDoA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For any packages that Colab does not provide auotmatically you will also need to install these below, e.g.:\n",
    "\n",
    "# !pip install ipympl\n",
    "#! pip install torch\n",
    "# ! pip install swifter\n",
    "# !wget https://cs.rochester.edu/u/nhossain/semeval-2020-task-7-dataset.zip -P data/\n",
    "# !unzip data/semeval-2020-task-7-dataset.zip\n",
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-01 12:21:20--  https://docs.google.com/uc?export=download&confirm=L_US&id=1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm\n",
      "Resolving docs.google.com (docs.google.com)... 2a00:1450:4009:817::200e, 172.217.169.14\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4009:817::200e|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0o-80-docs.googleusercontent.com/docs/securesc/1ibjbqmv2pnkirj6blrql7f5ftjp04cg/9rs149kpjsel6chmcn082f9bcf7401ko/1614601275000/16703018712613523847/08769474311849947251Z/1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm?e=download [following]\n",
      "--2021-03-01 12:21:20--  https://doc-0o-80-docs.googleusercontent.com/docs/securesc/1ibjbqmv2pnkirj6blrql7f5ftjp04cg/9rs149kpjsel6chmcn082f9bcf7401ko/1614601275000/16703018712613523847/08769474311849947251Z/1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm?e=download\n",
      "Resolving doc-0o-80-docs.googleusercontent.com (doc-0o-80-docs.googleusercontent.com)... 2a00:1450:4009:806::2001, 216.58.204.65\n",
      "Connecting to doc-0o-80-docs.googleusercontent.com (doc-0o-80-docs.googleusercontent.com)|2a00:1450:4009:806::2001|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=3gtpbrrahtmcu&continue=https://doc-0o-80-docs.googleusercontent.com/docs/securesc/1ibjbqmv2pnkirj6blrql7f5ftjp04cg/9rs149kpjsel6chmcn082f9bcf7401ko/1614601275000/16703018712613523847/08769474311849947251Z/1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm?e%3Ddownload&hash=78acuqlm3l0m3bd0kdu4rbf4dl3fol50 [following]\n",
      "--2021-03-01 12:21:20--  https://docs.google.com/nonceSigner?nonce=3gtpbrrahtmcu&continue=https://doc-0o-80-docs.googleusercontent.com/docs/securesc/1ibjbqmv2pnkirj6blrql7f5ftjp04cg/9rs149kpjsel6chmcn082f9bcf7401ko/1614601275000/16703018712613523847/08769474311849947251Z/1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm?e%3Ddownload&hash=78acuqlm3l0m3bd0kdu4rbf4dl3fol50\n",
      "Connecting to docs.google.com (docs.google.com)|2a00:1450:4009:817::200e|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0o-80-docs.googleusercontent.com/docs/securesc/1ibjbqmv2pnkirj6blrql7f5ftjp04cg/9rs149kpjsel6chmcn082f9bcf7401ko/1614601275000/16703018712613523847/08769474311849947251Z/1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm?e=download&nonce=3gtpbrrahtmcu&user=08769474311849947251Z&hash=vb4f106nfrnu0m7s2t9lteqel9bu9lc0 [following]\n",
      "--2021-03-01 12:21:20--  https://doc-0o-80-docs.googleusercontent.com/docs/securesc/1ibjbqmv2pnkirj6blrql7f5ftjp04cg/9rs149kpjsel6chmcn082f9bcf7401ko/1614601275000/16703018712613523847/08769474311849947251Z/1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm?e=download&nonce=3gtpbrrahtmcu&user=08769474311849947251Z&hash=vb4f106nfrnu0m7s2t9lteqel9bu9lc0\n",
      "Connecting to doc-0o-80-docs.googleusercontent.com (doc-0o-80-docs.googleusercontent.com)|2a00:1450:4009:806::2001|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘newsdata.zip’\n",
      "\n",
      "newsdata.zip            [  <=>               ]  25.44M  94.4MB/s    in 0.3s    \n",
      "\n",
      "2021-03-01 12:21:21 (94.4 MB/s) - ‘newsdata.zip’ saved [26677036]\n",
      "\n",
      "Archive:  newsdata.zip\n",
      "  inflating: News_Category_Dataset_v2.json  \n"
     ]
    }
   ],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1n2iImmVxmV9FtrNx2Z4FyVJI6bPsCTTm\" -O newsdata.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip newsdata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "id": "WX9TqmK7lDoK"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "tqdm_notebook.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X09jt8VRlDoM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed and device\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AqhlzLl6lDoO"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "funlines_df = pd.read_csv('data/train_funlines.csv')\n",
    "test_df = pd.read_csv('data/dev.csv')\n",
    "news_df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "all_headlines = list(train_df['original']) + list(funlines_df['original'])\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open(\"/vol/bitbucket/rp3317/data/glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "Casing of headlines makes traditional NER harder, train a statistical true caser then give to bert uncased model / use spacy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "upperalpha = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "punctuation = string.punctuation + \"‘’”“’'/-\\\\.\"\n",
    "hasCap = lambda x: (x[0] in upperalpha) or x[0].isdigit() if len(x) > 0 else False\n",
    "def percentageCap(sentence):\n",
    "    cleaned = re.sub(r\"<(.*)\\/>\", \"\\g<1>\", sentence).strip(punctuation).split(' ')\n",
    "    return (len([w for w in cleaned if hasCap(w)]) / len(cleaned), ' '.join(cleaned))\n",
    "\n",
    "cap_headlines = np.array([percentageCap(h) for h in all_headlines])\n",
    "fig, ax =plt.subplots(1,2, figsize=(15,5))\n",
    "caps = cap_headlines[:,0].astype(float)\n",
    "\n",
    "sns.histplot(caps, ax=ax[0])\n",
    "ax[0].set(xlabel='Percentage of words in headline that begin with capital letter', ylabel='Number of headlines')\n",
    "\n",
    "ax[1].set_xlabel('Percentage of words in headline that begin with capital letter')\n",
    "ax[1].set_ylabel('Density')\n",
    "ax[1]=plt.hist(caps,cumulative=True, density=True, bins=50, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesTruecaser, MosesTokenizer\n",
    "headlines_less_than = lambda limit: cap_headlines[caps < limit][:,1] \n",
    "headlines_more_than = lambda limit: cap_headlines[caps > limit][:,1] \n",
    "\n",
    "mtr = MosesTruecaser()\n",
    "mtok = MosesTokenizer(lang='en')\n",
    "tokenized_docs = [mtok.tokenize(line) for line in headlines_less_than(0.45)]\n",
    "truecase_results = mtr.train(tokenized_docs, save_to='big.truecasemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kellyanne Conway melts down under grilling by Fox news'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def truecase(headline):\n",
    "    s = mtr.truecase(headline.lower(), return_str=True)\n",
    "    return s\n",
    "\n",
    "truecase(np.random.choice(headlines_more_than(0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
    "# ! python -m spacy download en_core_web_lg\n",
    "# nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'pelosi', 'type': 'PER', 'start': 8, 'end': 14},\n",
       " {'word': 'trump', 'type': 'PER', 'start': 23, 'end': 28},\n",
       " {'word': 'state', 'type': 'ORG', 'start': 40, 'end': 45},\n",
       " {'word': 'union', 'type': 'ORG', 'start': 53, 'end': 58}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EntityTagger:\n",
    "    def __init__(self):\n",
    "        self.nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0)\n",
    "    \n",
    "    def _next_word_is_connected(self, results, i):\n",
    "        return (i < len(results) - 1) and \\\n",
    "                results[i+1]['word'].startswith('##') and \\\n",
    "                results[i]['end'] == results[i+1]['start'] \\\n",
    "        \n",
    "    def _process_results(self, results):\n",
    "        if not results:\n",
    "            return []\n",
    "        words = []\n",
    "        cur_word = \"\"\n",
    "        cur_start = results[0]['start']\n",
    "        for i, r in enumerate(results):\n",
    "            cur_word += re.sub(\"##(.*)\", \"\\g<1>\", r['word'])\n",
    "            if not self._next_word_is_connected(results, i):\n",
    "                words.append(dict(word=cur_word, type=r['entity'][2::], start=cur_start, end=r['end']))\n",
    "                cur_word = \"\"\n",
    "                cur_start = results[i+1]['start'] if i < len(results) - 1 else results[i]['start']\n",
    "            \n",
    "        return words\n",
    "            \n",
    "    def tag(self, text):\n",
    "        results = self.nlp(text)\n",
    "        return self._process_results(results)\n",
    "\n",
    "tagger = EntityTagger()\n",
    "# example = \"Visa , Mastercard , Stripe , and eBay all quit Facebook ’s Libra in one day\"\n",
    "example = np.random.choice(headlines_more_than(0.7))\n",
    "tagger.tag(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "class PreProcessor():\n",
    "    def __init__(self, remove_stops=False, lowercase=False, lemmatize=False, replace_edit=False):\n",
    "        self.remove_stops = remove_stops\n",
    "        self.lowercase = lowercase\n",
    "        self.lemmatize = lemmatize\n",
    "        self.replace_edit = replace_edit\n",
    "    \n",
    "    def process(self,row): \n",
    "        e, o = row['edit'], row['original']\n",
    "        if self.replace_edit:\n",
    "            replaced = re.sub(r\"<(.*)\\/>\", r\"\\g<1>\", row['original'])\n",
    "        else:\n",
    "            replaced = re.sub(r\"<(.*)\\/>\", e, o)\n",
    "        \n",
    "        replaced = replaced.translate(str.maketrans('','', punctuation))\n",
    "        if self.lowercase:\n",
    "            replaced = replaced.lower()\n",
    "        doc = nlp(replaced)\n",
    "        doc = [t for t in doc if t.text not in punctuation]\n",
    "        doc = [t for t in doc if not t.is_space] # Remove space tokens\n",
    "        if self.remove_stops:\n",
    "            doc = [t for t in doc if not (t.is_stop and t.text != e)] # Ensure edits aren't removed if they are stop words\n",
    "        if self.lemmatize:\n",
    "            st = \" \".join([t.lemma_ for t in doc])\n",
    "        else:\n",
    "            st = \" \".join([t.text for t in doc])\n",
    "            \n",
    "        return st\n",
    "# PreProcessor(remove_stops=True, lowercase=True, lemmatize=True) \\\n",
    "#     .process(dict(original=\"Trump announces ' precision strikes ' on Syria , decries ' <monster/> ' Assad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdf866d330748188970485fad5b559c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20532af3e3b54732af74f9bf8271f1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "preprocessor = PreProcessor(remove_stops=False)\n",
    "train_df['cleaned'] = train_df.progress_apply(preprocessor.process, axis=1)\n",
    "train_df['entity_tags'] = train_df.progress_apply(lambda x: tagger.tag(x['cleaned']), axis=1)\n",
    "with open(\"traindf.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_entities(threshold=8):\n",
    "    unique = defaultdict(lambda: defaultdict(int))\n",
    "    for i, entities in enumerate(train_df['entity_tags']):\n",
    "        for e in entities:\n",
    "            unique[e['type']][e['word']] += 1\n",
    "    \n",
    "    thresholded = defaultdict(lambda: defaultdict(int))\n",
    "    for k, v in unique.items():\n",
    "        for ent, count in v.items():\n",
    "            if count >= threshold:\n",
    "                thresholded[k][ent] = count\n",
    "    \n",
    "    return thresholded\n",
    "entities = get_unique_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b7539a8bd6468290bbbb87d2d08bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Occurences')"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = np.array(sorted(entities['PER'].items(), key=lambda x: x[1])[::-1])\n",
    "counts = names[:,1].astype(int)\n",
    "# for i, c in enumerate(counts):\n",
    "#     print(f\"{names[i][0]} - {c * 100 / sum(counts):.2f}%\")\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.bar(names[:,0][2:30],counts[2:30])\n",
    "fig.axes[0].tick_params(axis=\"x\", labelsize=9)\n",
    "fig.axes[0].tick_params(axis=\"y\", labelsize=12)\n",
    "fig.axes[0].set_xlabel(\"People\")\n",
    "fig.axes[0].set_ylabel(\"Occurences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d048d45ed7b4a24b0c36a782c0de249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['cleaned_no_stops'] = train_df.progress_apply(PreProcessor(remove_stops=True, lowercase=True, lemmatize=True).process, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ce63a1641145bf98b22f4c9b5fa131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['cleaned_replaced'] = train_df.progress_apply(PreProcessor(remove_stops=True, lowercase=True, lemmatize=True, replace_edit=True).process, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9652it [00:56, 169.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def all_entities():\n",
    "    all_entities = set()\n",
    "    for k in entities.keys():\n",
    "        for e in entities[k]:\n",
    "            all_entities.add(e)\n",
    "    return all_entities\n",
    "\n",
    "def getNearbyWords():\n",
    "    nearby_words = defaultdict(set)\n",
    "    counts = defaultdict(int)\n",
    "    for i, w in tqdm(enumerate(train_df['cleaned_no_stops'])):\n",
    "        tags = train_df.iloc[i]['entity_tags']\n",
    "        edit = nlp(train_df.iloc[i]['edit'].lower())[0].lemma_\n",
    "        ents_in_headline = set(tag['word'] for tag in tags)\n",
    "        nearby = set(w.split(' ')) - ents_in_headline\n",
    "        for w in ents_in_headline:\n",
    "            for n in nearby:\n",
    "                counts[(w,n)] += train_df.iloc[i]['meanGrade']**3\n",
    "            nearby_words[w] |= nearby\n",
    "        nearby_words[w].add(edit)\n",
    "        counts[(w,edit)] += train_df.iloc[i]['meanGrade']**3\n",
    "    return nearby_words, counts\n",
    "\n",
    "all_entities = all_entities()\n",
    "nearby_words, counts = getNearbyWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "def getEmbs(ent, three_dims=False, topn=20, plot=False):\n",
    "    if ent not in nearby_words:\n",
    "        print(\"Not valid ent\")\n",
    "        return\n",
    "    words_to_plot = [w for w in nearby_words[ent] if w in embeddings_dict]\n",
    "    if not words_to_plot:\n",
    "        print(\"No words to plot\")\n",
    "        return\n",
    "    words_to_plot = sorted(words_to_plot, key=lambda w: -counts[(ent, w)])[:topn]\n",
    "    vectors = np.array([embeddings_dict[w] for w in words_to_plot])\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        points = PCA(random_state=0).fit_transform(vectors)[:,:3]\n",
    "\n",
    "        if three_dims:\n",
    "            ax = Axes3D(fig)\n",
    "            ax.scatter(points[:,0], points[:,1], points[:,2])\n",
    "            for i, txt in enumerate(words_to_plot):\n",
    "                ax.text(points[i,0], points[i,1], points[i,2], txt)\n",
    "        else:\n",
    "            ax.scatter(points[:,0], points[:,1])\n",
    "            for i, txt in enumerate(words_to_plot):\n",
    "                ax.annotate(txt, (points[i,0], points[i,1]))\n",
    "        plt.show()\n",
    "    return words_to_plot, vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:05<00:00, 83.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def getClusterVectors(ent, plot=False, n_clusters=3, topn=50):\n",
    "    words, vectors = getEmbs(ent, topn=topn)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(vectors)\n",
    "    if plot:\n",
    "        points = PCA(random_state=0).fit_transform(np.concatenate((vectors,kmeans.cluster_centers_)))[:,:3]\n",
    "        cluster_colours = np.repeat(n_clusters, n_clusters)\n",
    "        colours = np.concatenate((kmeans.labels_, cluster_colours))\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(points[:,0], points[:,1], c=colours)\n",
    "        for i, txt in enumerate(words + [f'CLUSTER {i+1}' for i in range(n_clusters)]):\n",
    "            kwargs = {}\n",
    "            if 'CLUSTER ' in txt:\n",
    "                kwargs = {'weight': 'bold'}\n",
    "            ax.annotate(txt, (points[i,0], points[i,1]), **kwargs)\n",
    "        plt.show()\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "entity_clusters = {ent: getClusterVectors(ent, n_clusters=2, topn=10) for ent in tqdm(all_entities)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615f9643727440d09d37ca2d07839db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aceb884aafd44c1bf18d9d7467aedfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def position_of_edit(row):\n",
    "    words = row['original'].split(' ')\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if re.match(\"<.*\\/>\", w):\n",
    "            return count\n",
    "        count += 1\n",
    "    return 1\n",
    "\n",
    "def len_headline(row):\n",
    "    return len(row['original'].split(' '))\n",
    "    \n",
    "def funniness_of_each_entity():\n",
    "    funniness = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    for i, ents in tqdm(enumerate(train_df['entity_tags'])):\n",
    "        for ent in ents:\n",
    "            funniness[ent['word']] += train_df.iloc[i]['meanGrade']\n",
    "            counts[ent['word']] += 1\n",
    "            \n",
    "    avg_funniness = dict()\n",
    "    for k, v in funniness.items():\n",
    "        avg_funniness[k] = funniness[k] / counts[k]\n",
    "    \n",
    "    return avg_funniness\n",
    "        \n",
    "train_df['edit_pos'] = train_df.progress_apply(position_of_edit, axis=1)\n",
    "train_df['len'] = train_df.progress_apply(len_headline, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9652it [00:03, 3143.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#  sorted(list(entity_funniness.items()), key=lambda x: -entity_funniness[x[0]])\n",
    "entity_funniness = funniness_of_each_entity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26850ba5206f4d6dacfda67519ea67ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-539-8049e178e731>:20: RuntimeWarning: invalid value encountered in power\n",
      "  score += (max_sim ** 0.25) * entity_funniness[ent] ** 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3129dcc806924373b5b20c23ba39b09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.14481284300571423"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def get_max_similarity(vec, clusters):\n",
    "    return np.max([similarity(vec, c) for c in clusters])\n",
    "\n",
    "def entity_funniness_score(row): \n",
    "    r = row['cleaned_replaced']\n",
    "    relevant_entities = [x['word'] for x in row['entity_tags'] if (x['word'] in entity_funniness) and (x['word'] in entity_clusters)]\n",
    "    score = 0\n",
    "    for w in r.split(' '):\n",
    "        if w not in embeddings_dict or w in relevant_entities:\n",
    "            continue\n",
    "        vec = embeddings_dict[w]\n",
    "        for ent in relevant_entities:\n",
    "#             if not (ent in entities['PER'] or ent in entities['LOC']):\n",
    "#                 continue\n",
    "            clusters = entity_clusters[ent]\n",
    "            max_sim = get_max_similarity(vec, clusters)\n",
    "            score += (max_sim ** 0.25) * entity_funniness[ent] ** 3\n",
    "    return score\n",
    "    \n",
    "train_df['entity_funniness_score'] = train_df.progress_apply(entity_funniness_score, axis=1)\n",
    "train_df['entity_funniness_score'].fillna(0, inplace=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(train_df['entity_funniness_score'].to_numpy(), bins=52)\n",
    "plt.show()\n",
    "train_df['entity_funniness_score'].corr(train_df['meanGrade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ffcc16ecb741ae9a1eccf39dc74729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6970fb8449f748e68a6be487068ea0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-0.15680708656808165"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance_of_edit_to_original(row):\n",
    "#     return np.random.randint(100) * 0.01\n",
    "    e = row['edit'].lower()\n",
    "    original = re.search('<(.*)\\/>', row['original']).group(1).lower()\n",
    "    if original in embeddings_dict and e in embeddings_dict:\n",
    "        return similarity(embeddings_dict[e], embeddings_dict[original])\n",
    "    return 0\n",
    "\n",
    "train_df['edit_similarity'] = train_df.progress_apply(distance_of_edit_to_original, axis=1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(train_df['edit_similarity'].to_numpy(), bins=52)\n",
    "plt.show()\n",
    "train_df['edit_similarity'].corr(train_df['meanGrade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original</th>\n",
       "      <th>edit</th>\n",
       "      <th>grades</th>\n",
       "      <th>meanGrade</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>entity_tags</th>\n",
       "      <th>cleaned_no_stops</th>\n",
       "      <th>edit_pos</th>\n",
       "      <th>len</th>\n",
       "      <th>cleaned_replaced</th>\n",
       "      <th>entity_funniness_score</th>\n",
       "      <th>edit_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14530</td>\n",
       "      <td>France is ‘ hunting down its citizens who join...</td>\n",
       "      <td>twins</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>France is hunting down its citizens who joined...</td>\n",
       "      <td>[{'word': 'france', 'type': 'LOC', 'start': 0,...</td>\n",
       "      <td>france hunt citizen join twin trial iraq</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>france hunt citizen join isis trial iraq</td>\n",
       "      <td>6.842978</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13034</td>\n",
       "      <td>Pentagon claims 2,000 % increase in Russian tr...</td>\n",
       "      <td>bowling</td>\n",
       "      <td>33110</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Pentagon claims 2000 increase in Russian troll...</td>\n",
       "      <td>[{'word': 'pentagon', 'type': 'ORG', 'start': ...</td>\n",
       "      <td>pentagon claim 2000 increase russian troll bow...</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>pentagon claim 2000 increase russian troll syr...</td>\n",
       "      <td>15.329575</td>\n",
       "      <td>-0.019825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8731</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>party</td>\n",
       "      <td>22100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iceland PM Calls Snap Vote as Pedophile Furor ...</td>\n",
       "      <td>[{'word': 'iceland', 'type': 'LOC', 'start': 0...</td>\n",
       "      <td>iceland pm call snap vote pedophile furor cras...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>iceland pm call snap vote pedophile furor cras...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>In an apparent first , Iran and Israel &lt;engage...</td>\n",
       "      <td>slap</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>In an apparent first Iran and Israel engage ea...</td>\n",
       "      <td>[{'word': 'iran', 'type': 'LOC', 'start': 21, ...</td>\n",
       "      <td>apparent iran israel slap militarily</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>apparent iran israel engage militarily</td>\n",
       "      <td>3.352819</td>\n",
       "      <td>0.169812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6164</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled &lt;Vi...</td>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Trump was told weeks ago that Flynn misled Vic...</td>\n",
       "      <td>[{'word': 'trump', 'type': 'PER', 'start': 0, ...</td>\n",
       "      <td>trump tell week ago flynn mislead school presi...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>trump tell week ago flynn mislead vice president</td>\n",
       "      <td>10.341708</td>\n",
       "      <td>0.384219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           original     edit  grades  \\\n",
       "0  14530  France is ‘ hunting down its citizens who join...    twins   10000   \n",
       "1  13034  Pentagon claims 2,000 % increase in Russian tr...  bowling   33110   \n",
       "2   8731  Iceland PM Calls Snap Vote as Pedophile Furor ...    party   22100   \n",
       "3     76  In an apparent first , Iran and Israel <engage...     slap   20000   \n",
       "4   6164  Trump was told weeks ago that Flynn misled <Vi...   school       0   \n",
       "\n",
       "   meanGrade                                            cleaned  \\\n",
       "0        0.2  France is hunting down its citizens who joined...   \n",
       "1        1.6  Pentagon claims 2000 increase in Russian troll...   \n",
       "2        1.0  Iceland PM Calls Snap Vote as Pedophile Furor ...   \n",
       "3        0.4  In an apparent first Iran and Israel engage ea...   \n",
       "4        0.0  Trump was told weeks ago that Flynn misled Vic...   \n",
       "\n",
       "                                         entity_tags  \\\n",
       "0  [{'word': 'france', 'type': 'LOC', 'start': 0,...   \n",
       "1  [{'word': 'pentagon', 'type': 'ORG', 'start': ...   \n",
       "2  [{'word': 'iceland', 'type': 'LOC', 'start': 0...   \n",
       "3  [{'word': 'iran', 'type': 'LOC', 'start': 21, ...   \n",
       "4  [{'word': 'trump', 'type': 'PER', 'start': 0, ...   \n",
       "\n",
       "                                    cleaned_no_stops  edit_pos  len  \\\n",
       "0           france hunt citizen join twin trial iraq         9   15   \n",
       "1  pentagon claim 2000 increase russian troll bow...         9   17   \n",
       "2  iceland pm call snap vote pedophile furor cras...         9   11   \n",
       "3               apparent iran israel slap militarily         8   12   \n",
       "4  trump tell week ago flynn mislead school presi...         8   11   \n",
       "\n",
       "                                    cleaned_replaced  entity_funniness_score  \\\n",
       "0           france hunt citizen join isis trial iraq                6.842978   \n",
       "1  pentagon claim 2000 increase russian troll syr...               15.329575   \n",
       "2  iceland pm call snap vote pedophile furor cras...                0.000000   \n",
       "3             apparent iran israel engage militarily                3.352819   \n",
       "4   trump tell week ago flynn mislead vice president               10.341708   \n",
       "\n",
       "   edit_similarity  \n",
       "0         0.064800  \n",
       "1        -0.019825  \n",
       "2         0.830763  \n",
       "3         0.169812  \n",
       "4         0.384219  "
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a60d2e289cb4793a04622e262fb05d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c6cd1a99484e148986ce1996826f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de864b16e1b64fdc8c368e7671c38bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893455bffe88441383a6f396b70a58fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a52a9094ce406a9f5efdb9ce2da3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-539-8049e178e731>:20: RuntimeWarning: invalid value encountered in power\n",
      "  score += (max_sim ** 0.25) * entity_funniness[ent] ** 3\n"
     ]
    }
   ],
   "source": [
    "test_df['cleaned'] = test_df.progress_apply(preprocessor.process, axis=1)\n",
    "test_df['entity_tags'] = test_df.progress_apply(lambda x: tagger.tag(x['cleaned']), axis=1)\n",
    "test_df['cleaned_no_stops'] = test_df.progress_apply(PreProcessor(remove_stops=True, lowercase=True, lemmatize=True).process, axis=1)\n",
    "test_df['cleaned_replaced'] = test_df.progress_apply(PreProcessor(remove_stops=True, lowercase=True, lemmatize=True, replace_edit=True).process, axis=1)\n",
    "test_df['entity_funniness_score'] = test_df.progress_apply(entity_funniness_score, axis=1)\n",
    "test_df['entity_funniness_score'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 100)  \n",
    "        self.fc2 = nn.Linear(100, 100)  \n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - TL : 0.384 | TRAIN RMSE: 0.620 | VL : 0.341 | VAL RMSE : 0.584\n",
      "Epoch 1   - TL : 0.352 | TRAIN RMSE: 0.593 | VL : 0.346 | VAL RMSE : 0.588\n",
      "Epoch 2   - TL : 0.347 | TRAIN RMSE: 0.589 | VL : 0.343 | VAL RMSE : 0.586\n",
      "Epoch 3   - TL : 0.343 | TRAIN RMSE: 0.586 | VL : 0.343 | VAL RMSE : 0.585\n",
      "Epoch 4   - TL : 0.342 | TRAIN RMSE: 0.585 | VL : 0.342 | VAL RMSE : 0.585\n",
      "Epoch 5   - TL : 0.341 | TRAIN RMSE: 0.584 | VL : 0.341 | VAL RMSE : 0.584\n",
      "Epoch 6   - TL : 0.340 | TRAIN RMSE: 0.583 | VL : 0.341 | VAL RMSE : 0.584\n",
      "Epoch 7   - TL : 0.339 | TRAIN RMSE: 0.582 | VL : 0.340 | VAL RMSE : 0.583\n",
      "Epoch 8   - TL : 0.338 | TRAIN RMSE: 0.582 | VL : 0.340 | VAL RMSE : 0.583\n",
      "Epoch 9   - TL : 0.338 | TRAIN RMSE: 0.581 | VL : 0.339 | VAL RMSE : 0.583\n",
      "Epoch 10  - TL : 0.337 | TRAIN RMSE: 0.581 | VL : 0.339 | VAL RMSE : 0.582\n",
      "Epoch 11  - TL : 0.337 | TRAIN RMSE: 0.580 | VL : 0.339 | VAL RMSE : 0.583\n",
      "Epoch 12  - TL : 0.337 | TRAIN RMSE: 0.580 | VL : 0.339 | VAL RMSE : 0.582\n",
      "Epoch 13  - TL : 0.337 | TRAIN RMSE: 0.580 | VL : 0.338 | VAL RMSE : 0.582\n",
      "Epoch 14  - TL : 0.336 | TRAIN RMSE: 0.580 | VL : 0.338 | VAL RMSE : 0.582\n",
      "Epoch 15  - TL : 0.336 | TRAIN RMSE: 0.580 | VL : 0.338 | VAL RMSE : 0.582\n",
      "Epoch 16  - TL : 0.336 | TRAIN RMSE: 0.580 | VL : 0.338 | VAL RMSE : 0.582\n",
      "Epoch 17  - TL : 0.336 | TRAIN RMSE: 0.579 | VL : 0.338 | VAL RMSE : 0.582\n",
      "Epoch 18  - TL : 0.336 | TRAIN RMSE: 0.579 | VL : 0.338 | VAL RMSE : 0.582\n",
      "Epoch 19  - TL : 0.335 | TRAIN RMSE: 0.579 | VL : 0.339 | VAL RMSE : 0.582\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "# cols = ['edit_pos', 'len', 'edit_similarity', 'entity_funniness_score']\n",
    "cols = ['entity_funniness_score']\n",
    "training_data, dev_data, training_y, dev_y = train_test_split(train_df[cols], train_df['meanGrade'], test_size=0.1, random_state=42)\n",
    "train_X = torch.tensor(training_data.to_numpy()).float()\n",
    "val_X = torch.tensor(dev_data.to_numpy()).float()\n",
    "train_y = torch.tensor(training_y.to_numpy()).float()\n",
    "val_y = torch.tensor(dev_y.to_numpy()).float()\n",
    "test_X = torch.tensor(test_df[cols].to_numpy()).float()\n",
    "test_y = torch.tensor(test_df['meanGrade'].to_numpy()).float()\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(train_X, train_y)\n",
    "val_ds = torch.utils.data.TensorDataset(val_X, val_y)\n",
    "test_ds = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=8)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "def train(epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = net(X).squeeze(1)\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch_idx, (X, y) in enumerate(test_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = net(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                val_loss += loss.item()\n",
    "            vl = val_loss / len(test_loader)\n",
    "            val_losses.append(vl)\n",
    "            \n",
    "        tl = train_loss / len(train_loader)\n",
    "        train_losses.append(tl)\n",
    "        print(f\"Epoch {e:<3} - TL : {tl:.3f} | TRAIN RMSE: {np.sqrt(tl):.3f} | VL : {vl:.3f} | VAL RMSE : {np.sqrt(vl):.3f}\")\n",
    "    return train_losses, val_losses\n",
    "tl, vl = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5820224928055039"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for batch_idx, (X, y) in enumerate(test_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = net(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "\n",
    "np.sqrt(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "- 0 - not funny\n",
    "- 1 - slightly funny\n",
    "- 2 - moderately funny\n",
    "- 3 - very funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(train_df.meanGrade, kde=True, bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of people grading each example\n",
    "sns.countplot(x=\"grades\", data=pd.DataFrame(train_df.grades.apply(lambda x : len(str(x))), columns=['grades']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 -     holidays - Virginia <clashes/> bring attention to anti-fascist movement\n",
      "0.6 -     stealing - Wall Street 's top regulator is <resigning/> from the Fed\n",
      "0.2 -     pressure - Former intelligence director Clapper rips Comey 's firing , says US government is ‘ under <assault/> ’\n",
      "1.6 -      Dancing - Stop It — Trump Does n’t Do <Strategy/> \n",
      "1.8 -    custodian - This week InfoWars.com was offered White House <Press/> Credentials\n",
      "1.6 -          gum - China says it will never allow war or <chaos/> on its doorstep\n",
      "2.6 -      midlife - Chinese state media : Trump is ' wrong ' when he says we can fix the <North Korea/> crisis\n",
      "2.6 -          hug - Trump Holds First <Conversation/> with Putin in Oval Office\n",
      "2.4 -       sesame - Here 's how <Wall/> Street is reacting to Trump 's tax plan\n"
     ]
    }
   ],
   "source": [
    "def getExamplesForFunninessBetween(f1, f2):\n",
    "    mg = train_df['meanGrade']\n",
    "    return train_df[((mg < f2) & (mg > f1))]\n",
    "def showSamples(n):\n",
    "    for i in range(3):\n",
    "        d = getExamplesForFunninessBetween(i, i+1).sample(n)\n",
    "        for i in range(n):\n",
    "            cell = d.iloc[i]\n",
    "            print(f\"{cell['meanGrade']} - {cell['edit']:>12} - {cell['original']}\")\n",
    "\n",
    "showSamples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- Replace words in original with edit\n",
    "- Tokenise with spacy\n",
    "- Remove punctuation symbols\n",
    "- lowercase \n",
    "- Delete stop words\n",
    "\n",
    "## Ideas\n",
    "- Compare POS tag of old/new sentence\n",
    "- Make a language model on news articles and check perplexity/probability of edit = https://www.kaggle.com/rmisra/news-category-dataset\n",
    "- LDA model on real headlines - if its not part of the top 50 topics then probably out of domain\n",
    "- Check probability of all n-gram pairs surrounding edit\n",
    "- https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "punctuation = set(string.punctuation + '‘’”“')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def process(row): \n",
    "    e, o = row['edit'], row['original']\n",
    "    replaced = re.sub(r\"<.*\\/>\", e, o)\n",
    "    doc = nlp(replaced)\n",
    "    doc = [t for t in doc if t.text not in punctuation]\n",
    "    doc = [t for t in doc if not (t.is_stop and t.text != e)] # Ensure edits aren't removed if they are stop words\n",
    "    doc = [t for t in doc if not t.is_space] # Remove space tokens\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['edited'] = train_df.apply(process, axis=1)\n",
    "train_df.head(10)['edited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RCmF7xulDoP"
   },
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAgZW6K1lDoR"
   },
   "outputs": [],
   "source": [
    "# We define our training loop\n",
    "def train(train_iter, dev_iter, model, number_epoch):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "    print(\"Training model.\")\n",
    "    for epoch in range(1, number_epoch+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0  # Observations used for training so far\n",
    "\n",
    "        for batch in train_iter:\n",
    "            feature, target = batch\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(predictions, target)\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(dev_iter, model)\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
    "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzXeDgHmlDob"
   },
   "outputs": [],
   "source": [
    "# We evaluate performance on our dev set\n",
    "def eval(data_iter, model):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            feature, target = batch\n",
    "\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2_22fHHElDog"
   },
   "outputs": [],
   "source": [
    "# How we print the model performance\n",
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
    "    \"\"\"\n",
    "\n",
    "    sq_error = (output - target)**2\n",
    "\n",
    "    sse = np.sum(sq_error)\n",
    "    mse = np.mean(sq_error)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
    "\n",
    "    return sse, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LcxmqrKhlDoj"
   },
   "outputs": [],
   "source": [
    "def create_vocab(data):\n",
    "    \"\"\"\n",
    "    Creating a corpus of all the tokens used\n",
    "    \"\"\"\n",
    "    tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '): # simplest split is\n",
    "            tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "\n",
    "    return vocabulary, tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jzQ0KLXslDoq"
   },
   "outputs": [],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    We add padding to our minibatches and create tensors for our model\n",
    "    '''\n",
    "\n",
    "    batch_labels = [l for f, l in batch]\n",
    "    batch_features = [f for f, l in batch]\n",
    "\n",
    "    batch_features_len = [len(f) for f, l in batch]\n",
    "\n",
    "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
    "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    batch_labels = torch.FloatTensor(batch_labels)\n",
    "\n",
    "    return seq_tensor, batch_labels\n",
    "\n",
    "class Task1Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_data, labels):\n",
    "        self.x_train = train_data\n",
    "        self.y_train = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x_train[item], self.y_train[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sWaxTh2UlDoy"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
    "\n",
    "        out = self.hidden2label(lstm_out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bp9d32sOlDo6",
    "outputId": "9a477106-b5fb-4cf1-f1df-1ad21700d4c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab created.\n",
      "Model initialised.\n",
      "Dataloaders created.\n",
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 06 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 07 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
      "| Epoch: 08 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
      "| Epoch: 09 | Train Loss: 0.23 | Train MSE: 0.23 | Train RMSE: 0.48 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.61 |\n",
      "| Epoch: 10 | Train Loss: 0.22 | Train MSE: 0.22 | Train RMSE: 0.47 |         Val. Loss: 0.38 | Val. MSE: 0.38 |  Val. RMSE: 0.62 |\n"
     ]
    }
   ],
   "source": [
    "## Approach 1 code, using functions defined above:\n",
    "\n",
    "# We set our training data and test data\n",
    "training_data = train_df['original']\n",
    "test_data = test_df['original']\n",
    "\n",
    "# Creating word vectors\n",
    "training_vocab, training_tokenized_corpus = create_vocab(training_data)\n",
    "test_vocab, test_tokenized_corpus = create_vocab(test_data)\n",
    "\n",
    "# Creating joint vocab from test and train:\n",
    "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([training_data, test_data]))\n",
    "\n",
    "print(\"Vocab created.\")\n",
    "\n",
    "# We create representations for our tokens\n",
    "wvecs = [] # word vectors\n",
    "word2idx = [] # word2index\n",
    "idx2word = []\n",
    "\n",
    "# This is a large file, it will take a while to load in the memory!\n",
    "glove_file = '/vol/bitbucket/rp3317/data/glove.6B.100d.txt'\n",
    "with codecs.open(glove_file, 'r','utf-8') as f:\n",
    "  index = 1\n",
    "  for line in f.readlines():\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "      word = line.strip().split()[0]\n",
    "      if word in joint_vocab:\n",
    "          (word, vec) = (word,\n",
    "                     list(map(float,line.strip().split()[1:])))\n",
    "          wvecs.append(vec)\n",
    "          word2idx.append((word, index))\n",
    "          idx2word.append((index, word))\n",
    "          index += 1\n",
    "\n",
    "wvecs = np.array(wvecs)\n",
    "word2idx = dict(word2idx)\n",
    "idx2word = dict(idx2word)\n",
    "\n",
    "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in training_tokenized_corpus]\n",
    "\n",
    "# To avoid any sentences being empty (if no words match to our word embeddings)\n",
    "vectorized_seqs = [x if len(x) > 0 else [0] for x in vectorized_seqs]\n",
    "\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = BiLSTM(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "model.to(device)\n",
    "# We provide the model with our embeddings\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "feature = vectorized_seqs\n",
    "\n",
    "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
    "train_and_dev = Task1Dataset(feature, train_df['meanGrade'])\n",
    "\n",
    "train_examples = round(len(train_and_dev)*train_proportion)\n",
    "dev_examples = len(train_and_dev) - train_examples\n",
    "\n",
    "train_dataset, dev_dataset = random_split(train_and_dev,\n",
    "                                           (train_examples,\n",
    "                                            dev_examples))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "\n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train(train_loader, dev_loader, model, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdeFaoc3lDpK"
   },
   "source": [
    "#### Approach 2: No pre-trained representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46gm47T4lDpQ",
    "outputId": "200c6ad7-aa53-4e25-8939-11b46e6e6759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train performance:\n",
      "| MSE: 0.13 | RMSE: 0.37 |\n",
      "\n",
      "Dev performance:\n",
      "| MSE: 0.36 | RMSE: 0.60 |\n"
     ]
    }
   ],
   "source": [
    "train_and_dev = train_df['edit']\n",
    "\n",
    "training_data, dev_data, training_y, dev_y = train_test_split(train_df['edit'], train_df['meanGrade'],\n",
    "                                                                        test_size=(1-train_proportion),\n",
    "                                                                        random_state=42)\n",
    "\n",
    "# We train a Tf-idf model\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "train_counts = count_vect.fit_transform(training_data)\n",
    "transformer = TfidfTransformer().fit(train_counts)\n",
    "train_counts = transformer.transform(train_counts)\n",
    "regression_model = LinearRegression().fit(train_counts, training_y)\n",
    "\n",
    "# Train predictions\n",
    "predicted_train = regression_model.predict(train_counts)\n",
    "\n",
    "# Calculate Tf-idf using train and dev, and validate model on dev:\n",
    "test_and_test_counts = count_vect.transform(train_and_dev)\n",
    "transformer = TfidfTransformer().fit(test_and_test_counts)\n",
    "\n",
    "test_counts = count_vect.transform(dev_data)\n",
    "\n",
    "test_counts = transformer.transform(test_counts)\n",
    "\n",
    "# Dev predictions\n",
    "predicted = regression_model.predict(test_counts)\n",
    "\n",
    "# We run the evaluation:\n",
    "print(\"\\nTrain performance:\")\n",
    "sse, mse = model_performance(predicted_train, training_y, True)\n",
    "\n",
    "print(\"\\nDev performance:\")\n",
    "sse, mse = model_performance(predicted, dev_y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HyHwHkUlDpa"
   },
   "source": [
    "#### Baseline for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DA3q4o1lDpd",
    "outputId": "fa88780f-7ed0-4c2b-f2cf-bc0b2883626d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline performance:\n",
      "| MSE: 0.34 | RMSE: 0.58 |\n"
     ]
    }
   ],
   "source": [
    "# Baseline for the task\n",
    "pred_baseline = torch.zeros(len(dev_y)) + np.mean(training_y)\n",
    "print(\"\\nBaseline performance:\")\n",
    "sse, mse = model_performance(pred_baseline, dev_y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84nQDZyBlDpg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task_1_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
